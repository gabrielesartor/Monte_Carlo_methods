{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "monte_carlo_methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPp/GGYpmNvyRz4Ept7aX30"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE1Ri2F5M_6v",
        "colab_type": "text"
      },
      "source": [
        "## **Monte Carlo Methods**\n",
        "The aim of this notebook is to implement different Monte Carlo methods and verify how these algorithms work over simple domains. Monte Carlo methods are good to deal with environments whose dynamics is not completely known.\n",
        "These algorithm are able to learn effective policies from a direct experience with the environment (sampling the states and actions).\n",
        "\n",
        "**Try these methods with different parameters!**\n",
        "\n",
        "##Domain description\n",
        "The domain used in this notebook is the classical **chessboard**, over which our aim is to calculate the best policy to move towards a specific tile (goal).\n",
        "\n",
        "Possible **actions** and their decoding:\n",
        "*  left  = 0\n",
        "*  up    = 1\n",
        "*  right = 2\n",
        "*  down  = 3\n",
        "\n",
        "In this notebook, the **policy** is represented as a **grid**, showing in each possible state the action to execute (or the action having higher probability to be executed in the probabilistic case).\n",
        "\n",
        "The **reward** is -1 if the action is not feasible (the agent bumps on the border), +1 if the agent reaches the goal, 0 otherwise.\n",
        "\n",
        "In the following lines it is proposed an implementation of the algorithms explained in the chapter 5 of \"Reinforcement Learning: An Introduction\" (2018).\n",
        "\n",
        "\n",
        "Images links:\n",
        "1. http://www.howtoreinforcementlearning.com/wp-content/uploads/2019/07/image-8.png\n",
        "2. https://miro.medium.com/max/1490/1*nbe6oNqFQSIzB51Z8rDuag.pn\n",
        "3. https://marcinbogdanski.github.io/rl-sketchpad/RL_An_Introduction_2018/assets/0504_OnPolicy_MC_Ctrl.png\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvHEU4z_Ro7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import dependencies\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4kzecZnOnHb",
        "colab_type": "text"
      },
      "source": [
        "**First-visit MC prediction**\n",
        "\n",
        "\n",
        "![alt text](http://www.howtoreinforcementlearning.com/wp-content/uploads/2019/07/image-8.png) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDbwmyEVf_p2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#UTILITY FUNCTIONS FOR MC PREDICTION\n",
        "\n",
        "def compute_episode(rows, cols, policy_grid, T, goal):\n",
        "  \"\"\"\n",
        "  It returns an episode (S0,A0,R1,S1,A1,...,RT) computed applying the policy\n",
        "  passed as parameter and starting from a random state s.\n",
        "\n",
        "  :param rows: number of rows of the environment. \n",
        "  :param cols: number of cols of the environment. \n",
        "  :param policy_grid: grid showing the current greedy action to be executed.\n",
        "  :param T: number of steps of the episode.\n",
        "  :param goal: state (row,column) in which the agent receives the reward +1\n",
        "\n",
        "  :returns: the sequences of states, actions and rewards of the episode.\n",
        "  \"\"\"\n",
        "  S = []\n",
        "  A = []\n",
        "  R = []\n",
        "  s = (np.random.randint(0,rows-1),np.random.randint(0,cols-1)) #initialize S_0\n",
        "\n",
        "  for i in range(T):\n",
        "    S.append(s)\n",
        "    a = policy_grid[s[0],s[1]] #action of the policy to be executed\n",
        "\n",
        "    if a == 0: #if left\n",
        "      if s[1] == 0: #if the agent is on the left border\n",
        "        r = -1 #the agent bump the border and s doesn't change\n",
        "      else:\n",
        "        s = (s[0], s[1]-1)\n",
        "        r = 0\n",
        "\n",
        "    elif a == 1: #if up\n",
        "      if s[0] == 0: #if the agent is on the top border\n",
        "        r = -1 #the agent bump the border and s doesn't change\n",
        "      else:\n",
        "        s = (s[0]-1, s[1])\n",
        "        r = 0\n",
        "\n",
        "    elif a == 2: #if right\n",
        "      if s[1] == cols-1: #if the agent is on the right border\n",
        "        r = -1 #the agent bump the border and s doesn't change\n",
        "      else:\n",
        "        s = (s[0], s[1]+1)\n",
        "        r = 0\n",
        "\n",
        "    elif a == 3: #if down\n",
        "      if s[0] == rows-1: #if the agent is on the bottom border\n",
        "        r = -1 #the agent bump the border and s doesn't change\n",
        "      else:\n",
        "        s = (s[0]+1, s[1])\n",
        "        r = 0\n",
        "\n",
        "    else:\n",
        "      print(\"error in generate_actions!\")\n",
        "\n",
        "    if(s == goal):\n",
        "      r = 1\n",
        "\n",
        "    A.append(a)\n",
        "    R.append(r)\n",
        "    \n",
        "  return S, A, R\n",
        "\n",
        "def print_policy_grid(policy_grid):\n",
        "  \"\"\"\n",
        "  Given a policy grid showing the action to be executed in each state, it prints\n",
        "  the grid replacing the number of the action with an arrow representing the\n",
        "  direction of the action selected by the policy.\n",
        "\n",
        "  :param policy_grid: grid showing the current greedy action to be executed.\n",
        "  \"\"\"\n",
        "  num_rows = len(policy_grid)\n",
        "  num_cols = len(policy_grid[0])\n",
        "  policy_grid_arrows = np.chararray((num_rows,num_cols), unicode = True)\n",
        "\n",
        "  for r in range(num_rows):\n",
        "    for c in range(num_cols):\n",
        "      if policy_grid[r,c] == 0: #LEFT action\n",
        "        policy_grid_arrows[r,c] = u'\\u2190'\n",
        "      elif policy_grid[r,c] == 1: #UP action\n",
        "        policy_grid_arrows[r,c] = u'\\u2191'\n",
        "      elif policy_grid[r,c] == 2: #RIGHT action\n",
        "        policy_grid_arrows[r,c] = u'\\u2192'\n",
        "      elif policy_grid[r,c] == 3: #DOWN action\n",
        "        policy_grid_arrows[r,c] = u'\\u2193'\n",
        "      else:\n",
        "        policy_grid_arrows[r,c] = \"e\" #error\n",
        "  \n",
        "  print(\"Policy grid:\")\n",
        "  print(policy_grid_arrows)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtnxgzs3fdqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def first_visit_MC_prediction(policy = None, grid_rows = 6, grid_cols = 6, n_episodes = 1000, T = 10, gamma = 0.9, goal = (0,0)):\n",
        "  \"\"\"\n",
        "  It executes the algorithm of first-visit Monte Carlo prediction for a policy.\n",
        "\n",
        "  :param policy: policy to be evaluated (represented by a grid (grid_rows, grid_cols)).\n",
        "  :param rows: number of rows of the environment. \n",
        "  :param cols: number of cols of the environment. \n",
        "  :param n_episodes: number of episodes the algorithm will compute.\n",
        "  :param T: number of steps of each episode.\n",
        "  :param gamma: discount parameter.\n",
        "  :param goal: state (row,column) in which the agent receives the reward +1.\n",
        "  \"\"\"\n",
        "  #Initializations:\n",
        "\n",
        "  #Create policy to be evaluated (-1 if action is not feasible, 1 if the action brings to the goal, 0 otherwise)\n",
        "  policy_grid = policy\n",
        "  if(policy is None):\n",
        "    print(\"No policy passed as param... Generating random policy grid...\")\n",
        "    policy_grid = np.random.randint(0,3,size=(grid_rows,grid_cols))\n",
        "  print_policy_grid(policy_grid)\n",
        "\n",
        "  #initialize arbitrarily V(s)\n",
        "  V_s = np.random.uniform(0,0,(grid_rows,grid_cols))\n",
        "  print(\"\\nV(s):\")\n",
        "  print(V_s)\n",
        "\n",
        "  #Create the empty list Returns(s)\n",
        "  returns_s = {}\n",
        "  for r in range(grid_rows):\n",
        "    for c in range(grid_cols):\n",
        "      returns_s['s_'+str(r)+'_'+str(c)] = []\n",
        "\n",
        "  #Loop for each episode\n",
        "  for i in range(n_episodes):\n",
        "    G = 0\n",
        "    S, A, R = compute_episode(grid_rows, grid_cols, policy_grid, T, goal) #generate episode\n",
        "\n",
        "    for t in range(T-1,0,-1): #for each step of the episode\n",
        "      s = S[t]\n",
        "      G = gamma*G + R[t]\n",
        "      if s not in S[t-1:-1:-1]: #removing this check, first-visit becomes every-visit!\n",
        "        returns_s['s_'+str(s[0])+'_'+str(s[1])].append(G)\n",
        "        V_s[s[0],s[1]] = np.mean(returns_s['s_'+str(s[0])+'_'+str(s[1])])\n",
        "\n",
        "  print(\"\\nV(s) at the episode\", n_episodes,\"goal in (\",goal[0],\",\",goal[1],\")\\n\")\n",
        "  print(V_s)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL5AO3ZI-oaK",
        "colab_type": "code",
        "outputId": "098ca0c0-025f-41e9-d21c-dd78a684d705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "#TEST\n",
        "\n",
        "#create a random policy\n",
        "grid_rows,grid_cols = 6,6\n",
        "policy_grid = np.random.randint(0,3,size=(grid_rows,grid_cols))\n",
        "\n",
        "#execution of MC prediction\n",
        "first_visit_MC_prediction(grid_rows=5, grid_cols=5, n_episodes=5000, T=15)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No policy passed as param... Generating random policy grid...\n",
            "Policy grid:\n",
            "[['↑' '→' '↑' '←' '←']\n",
            " ['↑' '→' '→' '↑' '↑']\n",
            " ['→' '←' '→' '→' '→']\n",
            " ['↑' '→' '↑' '→' '←']\n",
            " ['←' '→' '↑' '←' '←']]\n",
            "\n",
            "V(s):\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "\n",
            "V(s) at the episode 5000 goal in ( 0 , 0 )\n",
            "\n",
            "[[ 5.04207951  0.         -4.85474807 -6.45682972  0.        ]\n",
            " [ 0.          0.         -5.00232075 -5.68899009  0.        ]\n",
            " [ 0.          0.         -5.69282773 -6.45828817 -4.75291667]\n",
            " [ 0.          0.         -5.00232075  0.          0.        ]\n",
            " [ 0.          0.          0.          0.          0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvRJcxuoQDls",
        "colab_type": "text"
      },
      "source": [
        "**Monte Carlo ES (Exploring Starts)**\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1490/1*nbe6oNqFQSIzB51Z8rDuag.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIt6pponsN-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#UTILITY FUNCTIONS FOR MC EXPLORING STARTS\n",
        "\n",
        "#compute episode with exploring starts assumption:\n",
        "#all pairs (s,a) have p > 0 to be S_0 and A_0\n",
        "def compute_episode_ES(rows, cols, policy_grid, T, goal):\n",
        "  \"\"\"\n",
        "  It returns an episode (S0,A0,R1,S1,A1,...,RT) computed applying the policy\n",
        "  passed as parameter and the exploring starts assumption.\n",
        "\n",
        "  :param rows: number of rows of the environment. \n",
        "  :param cols: number of cols of the environment. \n",
        "  :param policy_grid: grid showing the current greedy action to be executed.\n",
        "  :param T: number of steps of the episode.\n",
        "  :param goal: state (row,column) in which the agent receives the reward +1\n",
        "\n",
        "  :returns: the sequences of states, actions and rewards of the episode.\n",
        "  \"\"\"\n",
        "  S = []\n",
        "  A = []\n",
        "  R = []\n",
        "  s = (np.random.randint(0,rows-1),np.random.randint(0,cols-1)) #initialize S_0\n",
        "\n",
        "  for i in range(T):\n",
        "    S.append(s)\n",
        "\n",
        "    if i == 0:\n",
        "      a = np.random.randint(0,3) #exploring starts assumption\n",
        "    else:\n",
        "      a = policy_grid[s[0],s[1]] #action of the policy to be executed\n",
        "\n",
        "    if a == 0: #if left\n",
        "      if s[1] == 0: #if the agent is on the left border\n",
        "        r = -1 #the agent bump the border and s doesn't change\n",
        "      else:\n",
        "        s = (s[0], s[1]-1)\n",
        "        r = 0\n",
        "\n",
        "    elif a == 1: #if up\n",
        "      if s[0] == 0: #if the agent is on the top border\n",
        "        r = -1 #the agent bump the border and s doesn't change\n",
        "      else:\n",
        "        s = (s[0]-1, s[1])\n",
        "        r = 0\n",
        "\n",
        "    elif a == 2: #if right\n",
        "      if s[1] == cols-1: #if the agent is on the right border\n",
        "        r = -1 #the agent bump the border and s doesn't change\n",
        "      else:\n",
        "        s = (s[0], s[1]+1)\n",
        "        r = 0\n",
        "\n",
        "    elif a == 3: #if down\n",
        "      if s[0] == rows-1: #if the agent is on the bottom border\n",
        "        r = -1 #the agent bump the border and s doesn't change\n",
        "      else:\n",
        "        s = (s[0]+1, s[1])\n",
        "        r = 0\n",
        "\n",
        "    else:\n",
        "      print(\"error in generate_actions!\")\n",
        "\n",
        "    if(s == goal):\n",
        "      r = 1\n",
        "\n",
        "    A.append(a)\n",
        "    R.append(r)\n",
        "\n",
        "  return S, A, R\n",
        "\n",
        "#Calculates the best action to be selected in state s\n",
        "def argmax_Q_s_a(s, actions, Q_s_a):\n",
        "  \"\"\"\n",
        "  It calculates the best action which can be taken from s.\n",
        "\n",
        "  :param s: current state (row,column).\n",
        "  :param actions: list of available actions.\n",
        "  :param Q_s_a: dictionary containing the action-value of each pair (s,a).\n",
        "\n",
        "  :returns: action which maximizes the Q-value in the state s.\n",
        "  \"\"\"\n",
        "  best_a = actions[0]\n",
        "  best_value = Q_s_a[\"Q((\"+str(s[0])+\",\"+str(s[1])+\"),\"+str(actions[0])+\")\"]\n",
        "  for a in actions[1:]:\n",
        "    if Q_s_a[\"Q((\"+str(s[0])+\",\"+str(s[1])+\"),\"+str(actions[0])+\")\"] > best_value:\n",
        "      best_a = a\n",
        "      best_value = Q_s_a[\"Q((\"+str(s[0])+\",\"+str(s[1])+\"),\"+str(actions[0])+\")\"]\n",
        "\n",
        "  return best_a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGDCxQBxQKqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_policy_first_visit_MC_ES(grid_rows = 6, grid_cols = 6, n_episodes = 1000, T = 10, gamma = 0.9, goal = (0,0)):\n",
        "  \"\"\"\n",
        "  It implements the on-policy algorithm of Monte Carlo using the exploration\n",
        "  starts assumption.\n",
        "\n",
        "  :param grid_rows: number of rows of the environment. \n",
        "  :param grid_cols: number of cols of the environment. \n",
        "  :param n_episodes: number of episodes the algorithm will compute.\n",
        "  :param T: number of steps of each episode.\n",
        "  :param gamma: discount parameter.\n",
        "  :param goal: state (row,column) in which the agent receives the reward +1.\n",
        "  \"\"\"\n",
        "  #Simulation parameters\n",
        "  actions = [0, 1, 2, 3] #left, up, right, down\n",
        "\n",
        "  #Initialization:\n",
        "  #1.policy\n",
        "  policy_grid = np.random.randint(0,3,size=(grid_rows,grid_cols))\n",
        "  \n",
        "  #2.Q(s,a)\n",
        "  #3.Returns(s,a)\n",
        "  Q_s_a = {}\n",
        "  returns_s = {}\n",
        "  for r in range(grid_rows):\n",
        "    for c in range(grid_cols):\n",
        "      for a in actions:\n",
        "        Q_s_a[\"Q((\"+str(r)+\",\"+str(c)+\"),\"+str(a)+\")\"] = 0.\n",
        "        returns_s['returns('+str(r)+','+str(c)+\",\"+str(a)+\")\"] = []\n",
        "\n",
        "  print(\"Initial policy grid:\")\n",
        "  print_policy_grid(policy_grid)\n",
        "\n",
        "  #Loop for each episode\n",
        "  for i in range(n_episodes):\n",
        "    G = 0\n",
        "    #generate an episode from S_0 amd A_0 (randomly chosen)\n",
        "    S, A, R = compute_episode_ES(grid_rows,grid_cols,policy_grid,T,goal)\n",
        "\n",
        "    for t in range(T-1,0,-1): #for each step of the episode\n",
        "      s = S[t]\n",
        "      a = A[t]\n",
        "      G = gamma*G + R[t]\n",
        "      if s not in S[t-1:-1:-1]: #TODO: check on the pair S_t, A_t  -> removing this check, first-visit becomes every-visit!\n",
        "        returns_s['returns('+str(s[0])+','+str(s[1])+\",\"+str(a)+\")\"].append(G)\n",
        "        Q_s_a[\"Q((\"+str(s[0])+\",\"+str(s[1])+\"),\"+str(a)+\")\"] = np.mean(returns_s['returns('+str(s[0])+','+str(s[1])+\",\"+str(a)+\")\"])\n",
        "        policy_grid[s[0],s[1]] = argmax_Q_s_a(s, actions, Q_s_a) \n",
        "  \n",
        "  print(\"\\nFinal policy grid:\")\n",
        "  print_policy_grid(policy_grid)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZuKLzl4DJeF",
        "colab_type": "code",
        "outputId": "574fdb45-7c14-437e-b175-476338ec6c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "#TEST\n",
        "\n",
        "#Execution of the on-policy Monte Carlo ES (Exploring Starts)\n",
        "on_policy_first_visit_MC_ES(grid_rows=5, grid_cols=5,n_episodes = 5000, T = 10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial policy grid:\n",
            "Policy grid:\n",
            "[['→' '→' '←' '←' '↑']\n",
            " ['→' '→' '→' '←' '←']\n",
            " ['↑' '↑' '↑' '→' '→']\n",
            " ['↑' '→' '←' '↑' '↑']\n",
            " ['←' '→' '←' '←' '←']]\n",
            "\n",
            "Final policy grid:\n",
            "Policy grid:\n",
            "[['←' '←' '←' '←' '←']\n",
            " ['←' '←' '←' '←' '←']\n",
            " ['←' '←' '←' '←' '←']\n",
            " ['←' '←' '←' '←' '←']\n",
            " ['←' '→' '←' '←' '←']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgwujX7CQQRG",
        "colab_type": "text"
      },
      "source": [
        "**On-policy first-visit MC control**\n",
        "\n",
        "\n",
        "![alt text](https://marcinbogdanski.github.io/rl-sketchpad/RL_An_Introduction_2018/assets/0504_OnPolicy_MC_Ctrl.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB-wFhbv7GB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def select_action_epsilon_greedy(s, policy_grid, actions, epsilon):\n",
        "  \"\"\"\n",
        "  It selects an action using epsilon-greedy methods. With probability 1-epsilon,\n",
        "  it selects the current greedy action, with probability epsilon the action is \n",
        "  selected randomly.\n",
        "\n",
        "  :param s: current state (row,column).\n",
        "  :param policy_grid: grid showing the current greedy action to be executed.\n",
        "  :param actions: list of the available actions.\n",
        "  :param epsilon: parameter defining the exploration.\n",
        "\n",
        "  :returns: number of the action to execute.\n",
        "  \"\"\"\n",
        "  if(np.random.uniform() > epsilon):\n",
        "    return policy_grid[s[0],s[1]]\n",
        "  else:\n",
        "    return np.random.randint(0,len(actions)-1)\n",
        "\n",
        "def compute_episode_epsilon_greedy(rows, cols, policy_grid, actions, epsilon, T, goal):\n",
        "  S = []\n",
        "  A = []\n",
        "  R = []\n",
        "\n",
        "  s = (np.random.randint(0,rows-1),np.random.randint(0,cols-1)) #initialize S_0\n",
        "\n",
        "  for i in range(T):\n",
        "    S.append(s)\n",
        "    a = select_action_epsilon_greedy(s, policy_grid, actions, epsilon)\n",
        "\n",
        "    if a == 0: #if left\n",
        "      if s[1] == 0: #if the agent is on the left border\n",
        "        r = -1 #the agent bump the border and s doesn't change\n",
        "      else:\n",
        "        s = (s[0], s[1]-1)\n",
        "        r = 0\n",
        "\n",
        "    elif a == 1: #if up\n",
        "      if s[0] == 0: #if the agent is on the top border\n",
        "        r = -1 #the agent bump the border and s doesn't change\n",
        "      else:\n",
        "        s = (s[0]-1, s[1])\n",
        "        r = 0\n",
        "\n",
        "    elif a == 2: #if right\n",
        "      if s[1] == cols-1: #if the agent is on the right border\n",
        "        r = -1 #the agent bump the border and s doesn't change\n",
        "      else:\n",
        "        s = (s[0], s[1]+1)\n",
        "        r = 0\n",
        "\n",
        "    elif a == 3: #if down\n",
        "      if s[0] == rows-1: #if the agent is on the bottom border\n",
        "        r = -1 #the agent bump the border and s doesn't change\n",
        "      else:\n",
        "        s = (s[0]+1, s[1])\n",
        "        r = 0\n",
        "\n",
        "    else:\n",
        "      print(\"error in generate_actions!\")\n",
        "\n",
        "    if(s == goal):\n",
        "      r = 1\n",
        "\n",
        "    A.append(a)\n",
        "    R.append(r)\n",
        "\n",
        "  return S, A, R"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vixF2ba8QXVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_policy_first_visit_MC_control(grid_rows = 6, grid_cols = 6, n_episodes = 1000, T = 10, gamma = 0.9, epsilon = 0.5, goal = (0,0)):\n",
        "  \"\"\"\n",
        "  It implements the on-policy algorithm of Monte Carlo using for epsilon-soft\n",
        "  policies. \n",
        "\n",
        "  :param grid_rows: number of rows of the environment. \n",
        "  :param grid_cols: number of cols of the environment. \n",
        "  :param n_episodes: number of episodes the algorithm will compute.\n",
        "  :param T: number of steps of each episode.\n",
        "  :param gamma: discount parameter.\n",
        "  :param epsilon: exploration paramter.\n",
        "  :param goal: state (row,column) in which the agent receives the reward +1.\n",
        "  \"\"\"\n",
        "  actions = [0, 1, 2, 3] #left, up, right, down\n",
        "  \n",
        "  #1.Initialize arbitrary epsilon-soft policy\n",
        "  policy_grid = np.random.randint(0,3,size=(grid_rows,grid_cols))\n",
        "\n",
        "  #2.Q(s,a)\n",
        "  #3.Returns(s,a)\n",
        "  Q_s_a = {}\n",
        "  returns_s = {}\n",
        "  for r in range(grid_rows):\n",
        "    for c in range(grid_cols):\n",
        "      for a in actions:\n",
        "        Q_s_a[\"Q((\"+str(r)+\",\"+str(c)+\"),\"+str(a)+\")\"] = 0.\n",
        "        returns_s['returns('+str(r)+','+str(c)+\",\"+str(a)+\")\"] = []\n",
        "\n",
        "  print(\"Initial policy grid:\")\n",
        "  print_policy_grid(policy_grid)\n",
        "\n",
        "  #Loop for each episode\n",
        "  for i in range(n_episodes):\n",
        "    G = 0\n",
        "    #generate an episode using the policy (epsilon-greedy policy)\n",
        "    S, A, R = compute_episode_epsilon_greedy(grid_rows,grid_cols,policy_grid,actions,epsilon,T,goal)\n",
        "\n",
        "    for t in range(T-1,0,-1): #for each step of the episode\n",
        "      s = S[t]\n",
        "      a = A[t]\n",
        "      G = gamma*G + R[t]\n",
        "      if s not in S[t-1:-1:-1]: #TODO: check on the pair S_t, A_t  -> removing this check, first-visit becomes every-visit!\n",
        "        returns_s['returns('+str(s[0])+','+str(s[1])+\",\"+str(a)+\")\"].append(G)\n",
        "        Q_s_a[\"Q((\"+str(s[0])+\",\"+str(s[1])+\"),\"+str(a)+\")\"] = np.mean(returns_s['returns('+str(s[0])+','+str(s[1])+\",\"+str(a)+\")\"])\n",
        "        policy_grid[s[0],s[1]] = argmax_Q_s_a(s, actions, Q_s_a) \n",
        "\n",
        "  print(\"\\nFinal policy grid:\")\n",
        "  print_policy_grid(policy_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IEDRaYoaAiT",
        "colab_type": "code",
        "outputId": "49fecbf5-332d-4db1-ee11-ec862cd9b99f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "#TEST\n",
        "\n",
        "#Execute on-policy MC algorithm for epsilon-soft policies\n",
        "on_policy_first_visit_MC_control(grid_rows=5, grid_cols=5, n_episodes= 5000, T= 15, epsilon=0.5)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial policy grid:\n",
            "Policy grid:\n",
            "[['←' '↑' '→' '←' '←']\n",
            " ['→' '↑' '←' '→' '↑']\n",
            " ['↑' '→' '↑' '↑' '←']\n",
            " ['→' '↑' '↑' '→' '→']\n",
            " ['←' '→' '←' '↑' '←']]\n",
            "\n",
            "Final policy grid:\n",
            "Policy grid:\n",
            "[['←' '←' '←' '←' '←']\n",
            " ['←' '←' '←' '←' '←']\n",
            " ['←' '←' '←' '←' '←']\n",
            " ['←' '←' '←' '←' '←']\n",
            " ['←' '→' '←' '↑' '←']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRLNPFXzuyVc",
        "colab_type": "text"
      },
      "source": [
        "# **Monte Carlo off-policy methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_Uy8e0TaC-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def off_policy_MC_prediction(policy = None, grid_rows = 6, grid_cols = 6, n_episodes = 1000, T = 10, gamma = 0.9, goal = (0,0)):\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Zz1Y-jRxCOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TEST\n",
        "\n",
        "off_policy_MC_prediction()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evtJXBRkwrcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def off_policy_first_visit_MC_control(grid_rows = 6, grid_cols = 6, n_episodes = 1000, T = 10, gamma = 0.9, epsilon = 0.5, goal = (0,0)):\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVEW5-aaxHGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TEST\n",
        "\n",
        "off_policy_first_visit_MC_control()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}